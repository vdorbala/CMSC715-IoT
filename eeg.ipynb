{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vdorbala/CMSC715-IoT/blob/main/eeg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxCbSr5hohKq",
        "outputId": "8e743f32-fb10-41c2-c4de-a88af5f4f3db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgGNw2ZwfJkq",
        "outputId": "13dd40f9-7ed5-4b40-afd6-c34c8a8c418b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy==1.2.1 in /usr/local/lib/python3.7/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.2.1) (1.19.5)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install scipy==1.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "003ytsfaEuCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "994a64df-855b-4fbe-8801-34f6aabe7082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cnn_finetune in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from cnn_finetune) (4.62.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from cnn_finetune) (1.10.0+cu111)\n",
            "Requirement already satisfied: pretrainedmodels>=0.7.4 in /usr/local/lib/python3.7/dist-packages (from cnn_finetune) (0.7.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from cnn_finetune) (1.2.1)\n",
            "Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from cnn_finetune) (0.11.1+cu111)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels>=0.7.4->cnn_finetune) (2.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->cnn_finetune) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->cnn_finetune) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->cnn_finetune) (3.10.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels>=0.7.4->cnn_finetune) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install cnn_finetune"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/vlawhern/arl-eegmodels"
      ],
      "metadata": {
        "id": "5s9U6PQ-hbEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d546d28-6b3f-4fae-c5cb-8c9946e9c257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'arl-eegmodels' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd arl-eegmodels/\n",
        "!cp EEGModels.py ../\n",
        "%cd ../"
      ],
      "metadata": {
        "id": "4eMochSuiOkd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e03643be-c447-432a-d0f6-c6d1f299f442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/arl-eegmodels\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPCEs56Sbhye"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import weight_norm\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score as auc\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.interpolate import spline\n",
        "import os\n",
        "from os import path\n",
        "\n",
        "import csv\n",
        "from cnn_finetune import make_model\n",
        "\n",
        "from EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
        "from torchsummary import summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwM1KkcKdi6A"
      },
      "outputs": [],
      "source": [
        "beginning = time.time()\n",
        "\n",
        "use_gpu=0\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    use_gpu=1 \n",
        "    print (\"CUDA Available\")\n",
        "\n",
        "epochs = 20\n",
        "lr = 0.1\n",
        "\n",
        "batch_size = 4\n",
        "batch_size_test = 4\n",
        "epochs_num = epochs\n",
        "\n",
        "print (\"Batch size is {0}\".format(batch_size))\n",
        "print (\"Batch size test is {0}\".format(batch_size_test))\n",
        "print (\"Epoch size is {0}\".format(epochs_num))\n",
        "PATH = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDUmkDMDqdA2"
      },
      "outputs": [],
      "source": [
        "!cp EEGModels.py /content/drive/Shareddrives/IoT\\ Project\n",
        "%cd /content/drive/Shareddrives/IoT\\ Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7BRRJ4R5VzI"
      },
      "outputs": [],
      "source": [
        "labelfile = open('labelinfo.csv','w+')\n",
        "labelwriter = csv.writer(labelfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SraVI2-x4GqD"
      },
      "outputs": [],
      "source": [
        "for subdir, dirs, files in os.walk('labeled'):\n",
        "    for file in files:\n",
        "      if \"low\" in subdir:\n",
        "        labelwriter.writerow([str(file), \"low\"])\n",
        "      elif \"high\" in subdir:\n",
        "        labelwriter.writerow([str(file), \"high\"])\n",
        "      else:\n",
        "        labelwriter.writerow([str(file), \"normal\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3R7mlJrbGx97"
      },
      "outputs": [],
      "source": [
        "labelfile.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D71mV5IG1-Mo"
      },
      "outputs": [],
      "source": [
        "class eegdata(Dataset):\n",
        "\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.annotation_frame = pd.read_csv(csv_file, encoding = 'unicode_escape')\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotation_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        data_file = os.path.join(self.root_dir, self.annotation_frame.iloc[idx, 1], self.annotation_frame.iloc[idx, 0])\n",
        "        sc = StandardScaler()\n",
        "        data = np.load(data_file)\n",
        "\n",
        "        data = data[:1664,:].reshape(104,64)\n",
        "        data = sc.fit_transform(data)\n",
        "        \n",
        "        glucose_level = self.annotation_frame.iloc[idx, 1:]\n",
        "        if glucose_level is 'high':\n",
        "          glucose_level = [1,0,0]\n",
        "        elif glucose_level is 'normal':\n",
        "          glucose_level = [0,1,0]\n",
        "        else:\n",
        "          glucose_level = [0,0,1]\n",
        "        glucose_level = np.array([glucose_level])\n",
        "        glucose_level = glucose_level.astype('long').reshape(1, 3)\n",
        "        glucose_level = torch.LongTensor(glucose_level)\n",
        "        # glucose_level = glucose_level.astype('float').reshape(-1, 3)\n",
        "        sample = {'data': data, 'glucose': glucose_level}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwDdhHQhqWHM"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2Wx1j6JLjQG"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWod1Ug0BvkY"
      },
      "outputs": [],
      "source": [
        "def show_landmarks(data, glucose):\n",
        "    \"\"\"Show image with landmarks\"\"\"\n",
        "    plt.imshow(data)\n",
        "    plt.scatter(glucose[:, 0], glucose[:, 1], s=10, marker='.', c='r')\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWBbZttkPYxH"
      },
      "outputs": [],
      "source": [
        "glucose_dataset = eegdata(csv_file='/content/drive/Shareddrives/IoT Project/labelinfo.csv',\n",
        "                                    root_dir='/content/drive/Shareddrives/IoT Project/labeled')\n",
        "\n",
        "print(np.shape(glucose_dataset[0]['data']))\n",
        "print(np.shape(glucose_dataset))\n",
        "fig = plt.figure()\n",
        "\n",
        "for i in range(len(glucose_dataset)):\n",
        "    sample = glucose_dataset[i]\n",
        "\n",
        "    print(i, sample['data'].shape, sample['glucose'].shape)\n",
        "\n",
        "    ax = plt.subplot(1, 4, i + 1)\n",
        "    plt.tight_layout()\n",
        "    ax.set_title('Sample #{}'.format(i))\n",
        "    ax.axis('off')\n",
        "    show_landmarks(**sample)\n",
        "\n",
        "    if i == 3:\n",
        "        plt.show()\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBo_I3Kubhyo"
      },
      "source": [
        "# The Model\n",
        "\n",
        "We utilize EEGNet to train our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTJ9GiPAULLn"
      },
      "outputs": [],
      "source": [
        "# class TinyConv(nn.Module):\n",
        "#     \"\"\"Tiny convolutional neural network.\n",
        "#     Uses nn.Sequential to improve organization.\"\"\"\n",
        "#     def __init__(self):\n",
        "#         super(TinyConv, self).__init__()\n",
        "#         self.conv = nn.Sequential(\n",
        "#                             nn.Conv2d(1, 8, kernel_size = (3,3), stride=(3,3), padding=0),\n",
        "#                             nn.ReLU(inplace=True),\n",
        "                    \n",
        "#                             nn.Conv2d(8, 16, kernel_size = (3,3), stride=(3,3), padding=0),\n",
        "#                             nn.ReLU(inplace=True))\n",
        "#         self.fc = nn.Linear(in_features=16*3*3,out_features=3)\n",
        "    \n",
        "#     def forward(self, x):\n",
        "#         x = self.conv(x)\n",
        "#         x = torch.flatten(x,start_dim=1,end_dim=-1)\n",
        "#         x = self.fc(x)\n",
        "#         out = {'out':x}\n",
        "#         return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1t9KfWkbhyo"
      },
      "outputs": [],
      "source": [
        "# convolutional network model we will train to detect patterns in readings. For more information see my tutorial here.\n",
        "# model = make_model('resnet18', num_classes=3, pretrained=False, input_size=(104, 64))\n",
        "\n",
        "# class convmodel(nn.Module):\n",
        "    \n",
        "#     def __init__(self, out_classes, drop=0.5, d_linear=124):\n",
        "#         super().__init__()\n",
        "        \n",
        "#         self.conv2 = nn.Conv1d(1, 64, kernel_size=3, padding=0, stride=1)\n",
        "#         self.bn = nn.BatchNorm1d(64)\n",
        "#         self.pool = nn.MaxPool1d(2, stride=2)\n",
        "#         self.linear1 = nn.Linear(8128, d_linear)\n",
        "\n",
        "#         self.linear3 = nn.Linear(d_linear, out_classes)\n",
        "#         self.dropout1 = nn.Dropout(drop)\n",
        "#         self.dropout2 = nn.Dropout(drop)\n",
        "#         self.dropout3 = nn.Dropout(drop)\n",
        "        \n",
        "#         self.conv = nn.Sequential(self.conv2, nn.ReLU(inplace=True), self.bn,\\\n",
        "#                                     self.pool, self.dropout1) \n",
        "#         self.dense = nn.Sequential(self.linear1, nn.ReLU(inplace=True),self.dropout2,\\\n",
        "#                                     self.dropout3, self.linear3)\n",
        "#     def forward(self, x):\n",
        "#         bs = x.size(0)\n",
        "#         x = self.conv(x)\n",
        "#         x = x.view(bs, -1)\n",
        "#         output = self.dense(x)\n",
        "        \n",
        "#         return torch.sigmoid(output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EEGNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EEGNet, self).__init__()\n",
        "        self.T = 120\n",
        "        \n",
        "        # Layer 1\n",
        "        self.conv1 = nn.Conv2d(1, 16, (1, 64), padding = 0)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
        "        \n",
        "        # Layer 2\n",
        "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
        "        self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
        "        self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
        "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
        "        \n",
        "        # Layer 3\n",
        "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
        "        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
        "        self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
        "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
        "        \n",
        "        # FC Layer\n",
        "        # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
        "        # I have 120 timepoints. \n",
        "        self.fc1 = nn.Linear(4*2*6, 3)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        x = F.elu(self.conv1(x))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = F.dropout(x, 0.25)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        \n",
        "        # Layer 2\n",
        "        x = self.padding1(x)\n",
        "        x = F.elu(self.conv2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = F.dropout(x, 0.25)\n",
        "        x = self.pooling2(x)\n",
        "        \n",
        "        # Layer 3\n",
        "        x = self.padding2(x)\n",
        "        x = F.elu(self.conv3(x))\n",
        "        x = self.batchnorm3(x)\n",
        "        x = F.dropout(x, 0.25)\n",
        "        x = self.pooling3(x)\n",
        "        print(x.size())\n",
        "        # FC Layer\n",
        "        x = x.reshape(-1, 4*2*6)\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "model = EEGNet().cuda(0)\n",
        "summary(model, (1, 104, 64))\n",
        "print (model.forward(Variable(torch.Tensor(np.random.rand(1, 1, 104, 64)).cuda(0))))"
      ],
      "metadata": {
        "id": "rA7jxM0nwa0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvB0xNFbbhyp"
      },
      "outputs": [],
      "source": [
        "# model = convmodel(3).double()\n",
        "# print(model)\n",
        "# if USE_CUDA == 1:\n",
        "#     model = model.cuda()\n",
        "# optim = torch.optim.Adadelta(model_ft.parameters(), lr=1, eps=1e-10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaKgko-hboWc"
      },
      "outputs": [],
      "source": [
        "model_ft = model.to(device)\n",
        "\n",
        "if use_gpu == 1:\n",
        "    model_ft = nn.DataParallel(model_ft).cuda()\n",
        "\n",
        "if path.exists(PATH):\n",
        "    model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.BCELoss()\n",
        "\n",
        "optimizer = optim.SGD(model_ft.parameters(), lr=lr, momentum=0.9, weight_decay=0.005)\n",
        "# optimizer = optim.Adam(net.parameters())\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlTJQ0JdSAIE"
      },
      "outputs": [],
      "source": [
        "batch_size = 1\n",
        "validation_split = .2\n",
        "shuffle_dataset = True\n",
        "random_seed= 42\n",
        "\n",
        "# Creating data indices for training and validation splits:\n",
        "dataset_size = len(glucose_dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "if shuffle_dataset :\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Creating PT data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(glucose_dataset, batch_size=batch_size, num_workers=2,\n",
        "                                           sampler=train_sampler)\n",
        "validation_loader = torch.utils.data.DataLoader(glucose_dataset, batch_size=batch_size, num_workers=2,\n",
        "                                                sampler=valid_sampler)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgrWo_LeUFsT"
      },
      "outputs": [],
      "source": [
        "print(\"Glucose dataset size is {}. \\nTrain set size is {}. \\nValidation set size is {}.\".format(len(glucose_dataset),len(train_loader), len(validation_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnOSdLOrfJ6J"
      },
      "outputs": [],
      "source": [
        "trainlossarr = []\n",
        "testlossarr = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJhPFhttaLgh"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    global rdeftrain, trainlossarr\n",
        "    since = time.time()\n",
        "    total_loss = 0\n",
        "    total_size = 0\n",
        "    traintargetarr = []\n",
        "    trainoutputarr = []\n",
        "    model_ft.train()\n",
        "\n",
        "    for batch_idx, values in enumerate(train_loader):\n",
        "        \n",
        "        data, target = values['data'], values['glucose']\n",
        "\n",
        "        # print(target)\n",
        "        data = data.unsqueeze(1).float()\n",
        "        # data = data.view(-1, 4*2*6*batch_size)\n",
        "        target = target.view(-1, batch_size).float()\n",
        "\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model_ft(data)\n",
        "        \n",
        "        traintargetarr.append(target.tolist())\n",
        "        trainoutputarr.append(output.tolist())\n",
        "\n",
        "        output = torch.transpose(output, 0, 1)\n",
        "        print(data.size(), target.size(), output.size())\n",
        "\n",
        "        print(output, target)\n",
        "\n",
        "        loss = criterion(output, target.squeeze())\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        total_size += 1\n",
        "\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        # print((total_loss/total_size)*100)\n",
        "\n",
        "    # tta = np.array(traintargetarr)\n",
        "    # tta = [val for sublist in traintargetarr for val in sublist]\n",
        "    # tta = np.array(tta)\n",
        "    # toa = np.array(trainoutputarr)\n",
        "    # toa = [val for sublist in trainoutputarr for val in sublist]\n",
        "    # toa = np.array(toa)\n",
        "\n",
        "    trainlossarr.append((total_loss/total_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5Hb4VVMatIX"
      },
      "outputs": [],
      "source": [
        "def test():\n",
        "    global tta,toa,rdeftest,testlossarr\n",
        "    # model_ft.load_state_dict(torch.load(PATH))\n",
        "    model_ft.eval()\n",
        "    test_loss = 0\n",
        "    total_loss = 0\n",
        "    total_size = 0\n",
        "\n",
        "    testtargetarr = []\n",
        "    testoutputarr = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, values in enumerate(validation_loader):\n",
        "\n",
        "            data, target = values['data'], values['glucose']\n",
        "\n",
        "            data = data.unsqueeze(1).float()\n",
        "            target = target.view(-1,batch_size).float()\n",
        "\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            output = model_ft(data)\n",
        "\n",
        "            loss = criterion(output, target.squeeze())\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            total_size += 1\n",
        "        \n",
        "            testtargetarr.append(target.tolist())\n",
        "            testoutputarr.append(output.tolist())\n",
        "\n",
        "            # print (\"Loss is {}\".format(loss*100))\n",
        "\n",
        "    # tta = np.array(testtargetarr)\n",
        "    # tta = [val for sublist in testtargetarr for val in sublist]\n",
        "    # tta = np.array(tta)\n",
        "    # toa = np.array(testoutputarr)\n",
        "    # toa = [val for sublist in testoutputarr for val in sublist]\n",
        "    # toa = np.array(toa) \n",
        "    testlossarr.append(total_loss/total_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76Xoiz9ChHNy"
      },
      "outputs": [],
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test()\n",
        "    print (\"\\nEpoch number is {}.\".format(epoch))\n",
        "    print(\"Current train loss is {}. \\nCurrent test loss is {}.\".format((sum(trainlossarr)/epoch),sum(testlossarr)/epoch))\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Loss at epoch {} is {}%\".format(epoch,100*(sum(testlossarr)/epoch)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9X2wxV5Hbhyq"
      },
      "outputs": [],
      "source": [
        "time_elapsed = time.time() - beginning\n",
        "print('Process complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "trainlossarr = np.array(trainlossarr)\n",
        "testlossarr = np.array(testlossarr)\n",
        "\n",
        "file = open(\"eeg_model_vals.csv\",\"w+\")\n",
        "\n",
        "file.write(\"Train values are \\n\")\n",
        "\n",
        "for i in range(len(trainlossarr)):\n",
        "    file.write(\"{},\".format(trainlossarr[i]))\n",
        "\n",
        "file.write(\"\\n Test values are \\n\")\n",
        "\n",
        "for i in range(len(testlossarr)):\n",
        "    file.write(\"{},\".format(testlossarr[i]))\n",
        "\n",
        "file.close()\n",
        "\n",
        "t = np.arange(0,epochs_num,1)\n",
        "\n",
        "plt.plot(t,trainlossarr,t,testlossarr)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss value\")\n",
        "plt.title(\"Train and test losses (Normal)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0enU71Dbhyq"
      },
      "outputs": [],
      "source": [
        "# save general model\n",
        "!mkdir models\n",
        "torch.save(model.state_dict(), \"./models/Convnet_epoch1\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "eeg.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}