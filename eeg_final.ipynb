{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vdorbala/CMSC715-IoT/blob/main/eeg_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxCbSr5hohKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b796ce91-efbd-4528-8de1-280834849dc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgGNw2ZwfJkq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc47e801-0817-4eaa-966d-aa0bb68315af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy==1.2.1 in /usr/local/lib/python3.7/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.2.1) (1.19.5)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install scipy==1.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "003ytsfaEuCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff60acb8-2bbd-4d8d-c0fd-135c9fd766e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cnn_finetune in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from cnn_finetune) (1.2.1)\n",
            "Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from cnn_finetune) (0.11.1+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from cnn_finetune) (4.62.3)\n",
            "Requirement already satisfied: pretrainedmodels>=0.7.4 in /usr/local/lib/python3.7/dist-packages (from cnn_finetune) (0.7.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from cnn_finetune) (1.10.0+cu111)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels>=0.7.4->cnn_finetune) (2.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->cnn_finetune) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->cnn_finetune) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->cnn_finetune) (3.10.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels>=0.7.4->cnn_finetune) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install cnn_finetune"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/vlawhern/arl-eegmodels"
      ],
      "metadata": {
        "id": "5s9U6PQ-hbEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ce11b88-1618-4f87-f294-e949040aa6b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'arl-eegmodels' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd arl-eegmodels/\n",
        "!cp EEGModels.py ../\n",
        "%cd ../"
      ],
      "metadata": {
        "id": "4eMochSuiOkd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "869fe3ca-79a6-403d-e88c-e4cbe288b4d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/arl-eegmodels\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPCEs56Sbhye"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import weight_norm\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score as auc\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.interpolate import spline\n",
        "import os\n",
        "from os import path\n",
        "\n",
        "import csv\n",
        "from cnn_finetune import make_model\n",
        "\n",
        "from EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
        "from torchsummary import summary\n",
        "from pandas import Timestamp\n",
        "\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDUmkDMDqdA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb6826be-4a3f-4990-8542-c1ae81ceafa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/IoT Project\n"
          ]
        }
      ],
      "source": [
        "!cp EEGModels.py /content/drive/Shareddrives/IoT\\ Project\n",
        "%cd /content/drive/Shareddrives/IoT\\ Project/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip all.zip -d ./new_all/"
      ],
      "metadata": {
        "id": "fAh03tXZMMys",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "b1aa4321-bcf4-4059-db17-9101d4c3dbe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-74a4c9bc16c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unzip all.zip -d ./new_all/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    445\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m   result = _run_command(\n\u001b[0;32m--> 447\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    448\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    197\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    227\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;31m# TODO(b/115527726): Rather than sleep, poll for incoming messages from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;31m# the frontend in the same poll as for the output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7BRRJ4R5VzI"
      },
      "outputs": [],
      "source": [
        "labelfile = open('labelinfo.csv','w+')\n",
        "labelwriter = csv.writer(labelfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SraVI2-x4GqD"
      },
      "outputs": [],
      "source": [
        "for subdir, dirs, files in os.walk('./new_all'):\n",
        "    for file in files:\n",
        "      if \"low\" in subdir:\n",
        "        labelwriter.writerow([str(file), \"low\"])\n",
        "      elif \"high\" in subdir:\n",
        "        labelwriter.writerow([str(file), \"high\"])\n",
        "      else:\n",
        "        labelwriter.writerow([str(file), \"normal\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3R7mlJrbGx97"
      },
      "outputs": [],
      "source": [
        "labelfile.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D71mV5IG1-Mo"
      },
      "outputs": [],
      "source": [
        "class eegdata(Dataset):\n",
        "\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.annotation_frame = pd.read_csv(csv_file, encoding = 'unicode_escape')\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotation_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        data_file = os.path.join(self.root_dir, self.annotation_frame.iloc[idx, 1], self.annotation_frame.iloc[idx, 0])\n",
        "        sc = StandardScaler()\n",
        "        # data = pd.read_pickle(data_file + \".pkl\")\n",
        "        data = np.load(data_file, allow_pickle=True)\n",
        "\n",
        "        # data = data.to_numpy()\n",
        "        # data = data[:1664,:].reshape(104,64)\n",
        "        data = data[:1664,20:24]\n",
        "        data = sc.fit_transform(data)\n",
        "        \n",
        "        glucose_level = self.annotation_frame.iloc[idx, 1:]\n",
        "        if glucose_level is 'high':\n",
        "          glucose_level = [1,0,0]\n",
        "        elif glucose_level is 'normal':\n",
        "          glucose_level = [0,1,0]\n",
        "        else:\n",
        "          glucose_level = [0,0,1]\n",
        "        glucose_level = np.array([glucose_level])\n",
        "        glucose_level = glucose_level.astype('long').reshape(1, 3)\n",
        "        glucose_level = torch.LongTensor(glucose_level)\n",
        "        # glucose_level = glucose_level.astype('float').reshape(-1, 3)\n",
        "        sample = {'data': data, 'glucose': glucose_level}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwDdhHQhqWHM"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2Wx1j6JLjQG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c14ee7d-717f-4eaa-d6af-2524f528c658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " all.zip\t      labeled\t\t\t\t  models\n",
            " arl-eegmodels\t      labelinfo.csv\t\t\t  new_all\n",
            " EEGlab.h5\t     'Loss Graph 2021-12-19 204617.png'   __pycache__\n",
            " EEGModels.py\t     'Loss Graph 2021-12-19 205823.png'   resnet50.csv\n",
            " eeg_model_vals.csv  'Loss Graph 2021-12-19 212259.png'\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWod1Ug0BvkY"
      },
      "outputs": [],
      "source": [
        "def show_landmarks(data, glucose):\n",
        "    \"\"\"Show image with landmarks\"\"\"\n",
        "    plt.imshow(data)\n",
        "    plt.scatter(glucose[:, 0], glucose[:, 1], s=10, marker='.', c='r')\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWBbZttkPYxH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afec92de-a765-4e60-8f65-c75ac25be8ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1664, 4)\n",
            "[[ 0.39389359  0.67844869 -0.6301981  -0.38628329]\n",
            " [ 0.39389359  0.67844869 -0.6301981  -0.38628329]\n",
            " [ 0.39389359  0.67844869 -0.6301981  -0.38628329]\n",
            " ...\n",
            " [ 0.20556782 -1.0049936  -2.30114337 -2.32456914]\n",
            " [ 0.20556782 -1.0049936  -2.30114337 -2.32456914]\n",
            " [ 0.20556782 -1.0049936  -2.30114337 -2.32456914]]\n"
          ]
        }
      ],
      "source": [
        "glucose_dataset = eegdata(csv_file='/content/drive/Shareddrives/IoT Project/labelinfo.csv',\n",
        "                                    root_dir='/content/drive/Shareddrives/IoT Project/new_all/')\n",
        "\n",
        "print(np.shape(glucose_dataset[0]['data']))\n",
        "print(glucose_dataset[0]['data'])\n",
        "# print(np.shape(glucose_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "\n",
        "for i in range(len(glucose_dataset)):\n",
        "    sample = glucose_dataset[i]\n",
        "\n",
        "    print(i, sample['data'].shape, sample['glucose'].shape)\n",
        "\n",
        "    ax = plt.subplot(1, 4, i + 1)\n",
        "    plt.tight_layout()\n",
        "    ax.set_title('Sample #{}'.format(i))\n",
        "    ax.axis('off')\n",
        "    show_landmarks(**sample)\n",
        "\n",
        "    if i == 3:\n",
        "        plt.show()\n",
        "        break\n"
      ],
      "metadata": {
        "id": "vbPODblETHe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBo_I3Kubhyo"
      },
      "source": [
        "# The Model\n",
        "\n",
        "We utilize EEGNet to train our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTJ9GiPAULLn"
      },
      "outputs": [],
      "source": [
        "# class TinyConv(nn.Module):\n",
        "#     \"\"\"Tiny convolutional neural network.\n",
        "#     Uses nn.Sequential to improve organization.\"\"\"\n",
        "#     def __init__(self):\n",
        "#         super(TinyConv, self).__init__()\n",
        "#         self.conv = nn.Sequential(\n",
        "#                             nn.Conv2d(1, 8, kernel_size = (3,3), stride=(3,3), padding=0),\n",
        "#                             nn.ReLU(inplace=True),\n",
        "                    \n",
        "#                             nn.Conv2d(8, 16, kernel_size = (3,3), stride=(3,3), padding=0),\n",
        "#                             nn.ReLU(inplace=True))\n",
        "#         self.fc = nn.Linear(in_features=16*3*3,out_features=3)\n",
        "    \n",
        "#     def forward(self, x):\n",
        "#         x = self.conv(x)\n",
        "#         x = torch.flatten(x,start_dim=1,end_dim=-1)\n",
        "#         x = self.fc(x)\n",
        "#         out = {'out':x}\n",
        "#         return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwM1KkcKdi6A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46cdeaba-1ab0-4ab5-cf4f-24e6e1fb5774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available\n",
            "Batch size is 64\n",
            "Batch size test is 64\n",
            "Epoch size is 5\n"
          ]
        }
      ],
      "source": [
        "beginning = time.time()\n",
        "\n",
        "use_gpu=0\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    use_gpu=1 \n",
        "    print (\"CUDA Available\")\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "batch_size = 64\n",
        "batch_size_test = 64\n",
        "epochs_num = epochs\n",
        "\n",
        "print (\"Batch size is {0}\".format(batch_size))\n",
        "print (\"Batch size test is {0}\".format(batch_size_test))\n",
        "print (\"Epoch size is {0}\".format(epochs_num))\n",
        "PATH = './models/Convnet_epoch1'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EEGNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EEGNet, self).__init__()\n",
        "        self.T = 1664\n",
        "        \n",
        "        # Layer 1\n",
        "        self.conv1 = nn.Conv2d(1, 16, (1, 4), padding = 0)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
        "        \n",
        "        # Layer 2\n",
        "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
        "        self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
        "        self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
        "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
        "        \n",
        "        # Layer 3\n",
        "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
        "        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
        "        self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
        "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
        "        \n",
        "        # FC Layer\n",
        "        # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
        "        # I have 120 timepoints. \n",
        "        self.fc1 = nn.Linear(4*2*104, 3)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        x = F.elu(self.conv1(x))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = F.dropout(x, 0.25)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        \n",
        "        # Layer 2\n",
        "        x = self.padding1(x)\n",
        "        x = F.elu(self.conv2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = F.dropout(x, 0.25)\n",
        "        x = self.pooling2(x)\n",
        "        \n",
        "        # Layer 3\n",
        "        x = self.padding2(x)\n",
        "        x = F.elu(self.conv3(x))\n",
        "        x = self.batchnorm3(x)\n",
        "        x = F.dropout(x, 0.25)\n",
        "        x = self.pooling3(x)\n",
        "        # print(x.size())\n",
        "        # FC Layer\n",
        "        x = x.reshape(-1, 4*2*104)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "model = EEGNet().cuda(0)\n",
        "summary(model, (1, 1664, 4))\n",
        "print (model.forward(Variable(torch.Tensor(np.random.rand(1, 1, 1664, 4)).cuda(0))))"
      ],
      "metadata": {
        "id": "rA7jxM0nwa0q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bf6057a-85b2-46ad-ba10-62145694be09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 16, 1664, 1]              80\n",
            "       BatchNorm2d-2          [-1, 16, 1664, 1]              32\n",
            "         ZeroPad2d-3          [-1, 1, 17, 1697]               0\n",
            "            Conv2d-4          [-1, 4, 16, 1666]             260\n",
            "       BatchNorm2d-5          [-1, 4, 16, 1666]               8\n",
            "         MaxPool2d-6            [-1, 4, 4, 417]               0\n",
            "         ZeroPad2d-7           [-1, 4, 11, 420]               0\n",
            "            Conv2d-8            [-1, 4, 4, 417]             516\n",
            "       BatchNorm2d-9            [-1, 4, 4, 417]               8\n",
            "        MaxPool2d-10            [-1, 4, 2, 104]               0\n",
            "           Linear-11                    [-1, 3]           2,499\n",
            "================================================================\n",
            "Total params: 3,403\n",
            "Trainable params: 3,403\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.03\n",
            "Forward/backward pass size (MB): 2.55\n",
            "Params size (MB): 0.01\n",
            "Estimated Total Size (MB): 2.59\n",
            "----------------------------------------------------------------\n",
            "tensor([[0., 0., 0.]], device='cuda:0', grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaKgko-hboWc"
      },
      "outputs": [],
      "source": [
        "model_ft = model.to(device)\n",
        "\n",
        "if use_gpu == 1:\n",
        "    model_ft = nn.DataParallel(model_ft).cuda()\n",
        "\n",
        "# if path.exists(PATH):\n",
        "    # model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.BCELoss()\n",
        "\n",
        "optimizer = optim.SGD(model_ft.parameters(), lr=0.00001, momentum=0.999)\n",
        "# optimizer = optim.Adam(model_ft.parameters(), lr=0.001)\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlTJQ0JdSAIE"
      },
      "outputs": [],
      "source": [
        "validation_split = .2\n",
        "shuffle_dataset = True\n",
        "random_seed= 42\n",
        "\n",
        "# Creating data indices for training and validation splits:\n",
        "dataset_size = len(glucose_dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "if shuffle_dataset :\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Creating PT data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(glucose_dataset, batch_size=batch_size, num_workers=2,\n",
        "                                           sampler=train_sampler)\n",
        "validation_loader = torch.utils.data.DataLoader(glucose_dataset, batch_size=batch_size_test, num_workers=2,\n",
        "                                                sampler=valid_sampler)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgrWo_LeUFsT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4424b07-5905-4f68-ad33-03cba95ddeea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Glucose dataset size is 37077. \n",
            "Train set size is 464. \n",
            "Validation set size is 116.\n"
          ]
        }
      ],
      "source": [
        "print(\"Glucose dataset size is {}. \\nTrain set size is {}. \\nValidation set size is {}.\".format(len(glucose_dataset),len(train_loader), len(validation_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnOSdLOrfJ6J"
      },
      "outputs": [],
      "source": [
        "trainlossarr = []\n",
        "testlossarr = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJhPFhttaLgh"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    global rdeftrain, trainlossarr\n",
        "    since = time.time()\n",
        "    total_loss = 0\n",
        "    total_size = 0\n",
        "    traintargetarr = []\n",
        "    trainoutputarr = []\n",
        "    model_ft.train()\n",
        "\n",
        "    for batch_idx, values in enumerate(train_loader):\n",
        "        \n",
        "        data, target = values['data'], values['glucose']\n",
        "\n",
        "        # print(target)\n",
        "        data = data.unsqueeze(1).float()\n",
        "        # data = data.view(-1, 4*2*6*batch_size)\n",
        "        target = target.view(-1, 3).float()\n",
        "\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model_ft(data)\n",
        "        \n",
        "        traintargetarr.append(target.tolist())\n",
        "        trainoutputarr.append(output.tolist())\n",
        "\n",
        "        # output = torch.transpose(output, 0, 1)\n",
        "        # print(data.size(), target.size(), output.size())\n",
        "\n",
        "        # print(output, target)\n",
        "\n",
        "        loss = criterion(output, target.squeeze())\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        total_size += 1\n",
        "\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        # print((total_loss/total_size)*100)\n",
        "\n",
        "    # tta = np.array(traintargetarr)\n",
        "    # tta = [val for sublist in traintargetarr for val in sublist]\n",
        "    # tta = np.array(tta)\n",
        "    # toa = np.array(trainoutputarr)\n",
        "    # toa = [val for sublist in trainoutputarr for val in sublist]\n",
        "    # toa = np.array(toa)\n",
        "\n",
        "    trainlossarr.append((total_loss/total_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5Hb4VVMatIX"
      },
      "outputs": [],
      "source": [
        "def test():\n",
        "    global tta,toa,rdeftest,testlossarr\n",
        "    # model_ft.load_state_dict(torch.load(PATH))\n",
        "    model_ft.eval()\n",
        "    test_loss = 0\n",
        "    total_loss = 0\n",
        "    total_size = 0\n",
        "\n",
        "    testtargetarr = []\n",
        "    testoutputarr = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, values in enumerate(validation_loader):\n",
        "\n",
        "            data, target = values['data'], values['glucose']\n",
        "            \n",
        "            data = data.unsqueeze(1).float()\n",
        "            target = target.view(-1,3).float()\n",
        "\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            output = model_ft(data)\n",
        "\n",
        "            # print(data.size(), target.size(), output.size())\n",
        "\n",
        "            # print(output, target)\n",
        "\n",
        "            loss = criterion(output, target.squeeze())\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            total_size += 1\n",
        "        \n",
        "            testtargetarr.append(target.tolist())\n",
        "            testoutputarr.append(output.tolist())\n",
        "\n",
        "            print (\"Loss is {}\".format(loss*100))\n",
        "\n",
        "    # tta = np.array(testtargetarr)\n",
        "    # tta = [val for sublist in testtargetarr for val in sublist]\n",
        "    # tta = np.array(tta)\n",
        "    # toa = np.array(testoutputarr)\n",
        "    # toa = [val for sublist in testoutputarr for val in sublist]\n",
        "    # toa = np.array(toa) \n",
        "    testlossarr.append(total_loss/total_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76Xoiz9ChHNy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30044f48-6b91-499e-8d33-b8053bb072f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "\n",
            "Epoch number is 1.\n",
            "Current train loss is 0.0. \n",
            "Current test loss is 0.0.\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "\n",
            "Epoch number is 2.\n",
            "Current train loss is 0.0. \n",
            "Current test loss is 0.0.\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "\n",
            "Epoch number is 3.\n",
            "Current train loss is 0.0. \n",
            "Current test loss is 0.0.\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "\n",
            "Epoch number is 4.\n",
            "Current train loss is 0.0. \n",
            "Current test loss is 0.0.\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "Loss is -0.0\n",
            "\n",
            "Epoch number is 5.\n",
            "Current train loss is 0.0. \n",
            "Current test loss is 0.0.\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test()\n",
        "    print (\"\\nEpoch number is {}.\".format(epoch))\n",
        "    print(\"Current train loss is {}. \\nCurrent test loss is {}.\".format((sum(trainlossarr)/epoch),sum(testlossarr)/epoch))\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Loss at epoch {} is {}%\".format(epoch,100*(sum(testlossarr)/epoch)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9X2wxV5Hbhyq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "fa680173-3c4f-4aac-aa2c-e3f41ffb84ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process complete in 101m 39s\n",
            "(5,) (5,)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "time_elapsed = time.time() - beginning\n",
        "print('Process complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "trainlossarr = np.array(trainlossarr)\n",
        "testlossarr = np.array(testlossarr)\n",
        "\n",
        "print(np.shape(trainlossarr), np.shape(testlossarr))\n",
        "\n",
        "\n",
        "file = open(\"eeg_model_vals.csv\",\"w+\")\n",
        "\n",
        "file.write(\"Train values are \\n\")\n",
        "\n",
        "for i in range(len(trainlossarr)):\n",
        "    file.write(\"{},\".format(trainlossarr[i]))\n",
        "\n",
        "file.write(\"\\n Test values are \\n\")\n",
        "\n",
        "for i in range(len(testlossarr)):\n",
        "    file.write(\"{},\".format(testlossarr[i]))\n",
        "\n",
        "file.close()\n",
        "\n",
        "t = np.arange(0,epochs_num,1)\n",
        "\n",
        "plt.plot(t,trainlossarr,t,testlossarr)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss value\")\n",
        "plt.title(\"Train and Validation losses\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0enU71Dbhyq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8dab18d-589c-4bad-8c35-527f5ef0a0ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory models: File exists\n"
          ]
        }
      ],
      "source": [
        "# save general model\n",
        "!mkdir models\n",
        "torch.save(model.state_dict(), \"./models/raw_conv_{}\".format(time.strftime(\"%Y-%m-%d %H%M%S\")))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime"
      ],
      "metadata": {
        "id": "Zpg_oxX6b4Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.savefig(\"Loss Graph \"+ time.strftime(\"%Y-%m-%d %H%M%S\") + \".png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "JGlzQyclbtb7",
        "outputId": "de2927d3-28f9-4637-ee24-0dfc6cf6926e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model_ft(data)"
      ],
      "metadata": {
        "id": "gkEdPM3aJD5g"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "eeg-final.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}